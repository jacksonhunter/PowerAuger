
    Write-Host ""
    Write-Host "    ████████ ███████ ████████ ██████   █████  ████████ ██████   ██████  ███    ██ ██  ██████ " -ForegroundColor Cyan
    Write-Host "       ██    ██         ██    ██   ██ ██   ██    ██    ██   ██ ██    ██ ████   ██ ██ ██      " -ForegroundColor Cyan
    Write-Host "       ██    █████      ██    ██████  ███████    ██    ██████  ██    ██ ██ ██  ██ ██ ██      " -ForegroundColor Magenta
    Write-Host "       ██    ██         ██    ██   ██ ██   ██    ██    ██   ██ ██    ██ ██  ██ ██ ██ ██      " -ForegroundColor Magenta
    Write-Host "       ██    ███████    ██    ██   ██ ██   ██    ██    ██   ██  ██████  ██   ████ ██  ██████ " -ForegroundColor Red
    Write-Host ""
    Write-Host "    ╔═══════════════════════════════════════════════════════════════════════════════════╗" -ForegroundColor Yellow
    Write-Host "    ║                          >> NEURAL INTERFACE ACTIVE <<                            ║" -ForegroundColor Yellow
    Write-Host "    ║                        Advanced AI Computing Solutions                            ║" -ForegroundColor White
    Write-Host "    ╚═══════════════════════════════════════════════════════════════════════════════════╝" -ForegroundColor Yellow

Write-Host "AI is thinking... (model: $model, context: $context, complexity: $complexity)" -ForegroundColor Yellow
# Initialize global variables
if (-not $Global:AskHistory) { $Global:AskHistory = @() }
if (-not $Global:CurrentModel) { $Global:CurrentModel = "qwen3:4b-q4_K_M" }
if (-not $Global:OllamaServer) { $Global:OllamaServer = "http://192.168.50.194:11434" }

function Set-AskModel {
    param(
        [ValidateSet("fast", "code", "think", "qwen3:4b-q4_K_M", "qwen3-coder:30b", "qwen3:30b-a3b-thinking-2507-q4_K_M")]
        [string]$Model = "fast"
    )
    
    $Global:CurrentModel = switch ($Model) {
        "fast" { "qwen3:4b-q4_K_M" }
        "code" { "qwen3-coder:30b" }
        "think" { "qwen3:30b-a3b-thinking-2507-q4_K_M" }
        default { $Model }
    }
    
    Write-Host "Model set to: $Global:CurrentModel" -ForegroundColor Yellow
}

function Ask {
    param(
        [string]$question,
        [string]$context = "PowerShell",
        [string]$complexity = "auto",
        [string]$model = $Global:CurrentModel,
        [switch]$ShowThoughts,
        [string]$file
    )
    
    # Auto-detect complexity based on question length and content
    if ($complexity -eq "auto") {
        $length = $question.Length
        $hasCodeWords = $question -match '\b(script|function|class|algorithm|explain|analyze|compare|research|debug|optimize|refactor|create|build|implement)\b'
        
        # Fix the question splitting bug - only count actual questions, not trailing ?
        $questionParts = ($question -split '\?') | Where-Object { $_.Trim() -ne "" }
        $hasMultipleQuestions = $questionParts.Count -gt 1
        
        # Enhanced complexity detection
        $hasComplexKeywords = $question -match '\b(comprehensive|detailed|thorough|step-by-step|best practices|pros and cons|advantages and disadvantages)\b'
        $hasCodeContext = $question -match '\b(code|syntax|programming|development|implementation|architecture)\b'
        
        if ($length -lt 30 -and -not $hasCodeWords -and -not $hasComplexKeywords) {
            $complexity = "simple"
        } elseif ($length -gt 150 -or $hasComplexKeywords -or $hasMultipleQuestions -or ($hasCodeWords -and $hasCodeContext)) {
            $complexity = "complex"
        } else {
            $complexity = "medium"
        }
    }
    
    # Handle file input
    $fileContent = ""
    if ($file -and (Test-Path $file)) {
        $fileContent = Get-Content $file -Raw
        Write-Host "Loaded file: $file" -ForegroundColor Yellow
    }
    
    # Build context-aware prompt
    $contextualPrompt = switch ($complexity) {
        "simple" { 
            if ($context -eq "PowerShell") {
                "Give a brief PowerShell command or short answer: $question"
            } else {
                "Give a brief answer for $context context: $question"
            }
        }
        "medium" { 
            if ($fileContent) {
                "In $context context, here's a file:\n\n$fileContent\n\nQuestion: $question"
            } else {
                "In $context context, $question"
            }
        }
        "complex" { 
            if ($fileContent) {
                "Provide a detailed $context analysis of this file and answer the question with examples:\n\nFile content:\n$fileContent\n\nQuestion: $question"
            } else {
                "Provide a detailed $context explanation with examples and references: $question"
            }
        }
    }
    
    # Smart model selection based on complexity and context
    if ($model -eq $Global:CurrentModel -and $Global:CurrentModel -eq "qwen3:4b-q4_K_M") {
        # Auto-select best model for the task if using default fast model
        if ($complexity -eq "complex" -or $context -match "Analysis|Research|Think") {
            $model = "qwen3:30b-a3b-thinking-2507-q4_K_M"
            Write-Host "Auto-selected thinking model for complex task" -ForegroundColor Gray
        } elseif ($context -match "Code|Python|JavaScript|C#|Java|Go|Rust" -or $hasCodeWords) {
            $model = "qwen3-coder:30b"
            Write-Host "Auto-selected coding model for programming task" -ForegroundColor Gray
        }
        # else stick with fast model for simple/medium tasks
    }
    
    # Clean up any previous job/event
    if ($Global:LastAskJob) {
        Remove-Job $Global:LastAskJob -Force -ErrorAction SilentlyContinue
    }
    Get-EventSubscriber -SourceIdentifier "AskJobComplete" -ErrorAction SilentlyContinue | Unregister-Event
    
    # Start the background job with Ollama API
    $Global:LastAskJob = Start-Job -ScriptBlock {
        param($prompt, $originalQuestion, $ctx, $comp, $modelName, $serverUrl, $showThoughts)
        
        # Use Ollama's native API format
        $body = @{
            model = $modelName
            prompt = $prompt
            stream = $false
        } | ConvertTo-Json -Depth 3
        
        try {
            $response = Invoke-RestMethod -Uri "$serverUrl/api/generate" -Method Post -Body $body -ContentType "application/json"
            return @{
                question = $originalQuestion
                context = $ctx
                complexity = $comp
                model = $modelName
                fullResponse = $response.response
                timestamp = Get-Date
                showThoughts = $showThoughts
            }
        }
        catch {
            return @{
                question = $originalQuestion
                context = $ctx
                complexity = $comp
                model = $modelName
                fullResponse = "Error: $($_.Exception.Message)"
                timestamp = Get-Date
                showThoughts = $showThoughts
            }
        }
        
    } -ArgumentList $contextualPrompt, $question, $context, $complexity, $model, $Global:OllamaServer, $ShowThoughts.IsPresent
    
    # Register event for async response handling
    Register-ObjectEvent -InputObject $Global:LastAskJob -EventName StateChanged -SourceIdentifier "AskJobComplete" -Action {
        if ($Event.Sender.State -eq "Completed") {
            $result = Receive-Job -Job $Event.Sender
            
            # Extract thoughts and clean answer
            $thoughts = ""
            $cleanAnswer = $result.fullResponse
            
            if ($result.fullResponse -match '<think>([\s\S]*?)</think>') {
                $thoughts = $matches[1].Trim()
                $cleanAnswer = $result.fullResponse -replace '<think>[\s\S]*?</think>', '' -replace '^\s+|\s+$', ''
            }
            
            # Handle empty clean answer
            if ([string]::IsNullOrWhiteSpace($cleanAnswer)) {
                $cleanAnswer = $result.fullResponse
            }
            
            # Store in history with context info
            $Global:AskHistory += @{
                Index = $Global:AskHistory.Count + 1
                Question = $result.question
                Context = $result.context
                Complexity = $result.complexity
                Model = $result.model
                Answer = $cleanAnswer
                Thoughts = $thoughts
                Timestamp = $result.timestamp
            }
            
            # Display response
            Write-Host "`n=== AI Response ($($result.model)) ===" -ForegroundColor Cyan
            
            if ($result.showThoughts -and $thoughts) {
                Write-Host "`n--- Reasoning ---" -ForegroundColor Yellow
                Write-Host $thoughts -ForegroundColor Gray
                Write-Host "`n--- Answer ---" -ForegroundColor Yellow
            }
            
            Write-Host $cleanAnswer -ForegroundColor Green  
            Write-Host "========================================`n" -ForegroundColor Cyan
            
            # Clean up
            Remove-Job $Event.Sender -Force
            Unregister-Event -SourceIdentifier "AskJobComplete"
        }
    } | Out-Null
}

# Enhanced convenience functions with better model targeting
function Ask-Code { 
    param([string]$question, [string]$file)
    Ask $question -context "Code" -file $file -model "qwen3-coder:30b"
}

function Ask-Think { 
    param([string]$question, [switch]$ShowThoughts)
    Ask $question -context "Analysis" -ShowThoughts:$ShowThoughts -model "qwen3:30b-a3b-thinking-2507-q4_K_M"
}

function Ask-Fast { 
    param([string]$question)
    Ask $question -complexity "simple" -model "qwen3:4b-q4_K_M"
}

# Context-specific functions with smart model selection
function Ask-Python { 
    param([string]$question, [string]$file) 
    Ask $question -context "Python" -file $file -model "qwen3-coder:30b"
}
function Ask-Bash { 
    param([string]$question) 
    Ask $question -context "Bash" -model "qwen3-coder:30b"
}
function Ask-JS { 
    param([string]$question, [string]$file) 
    Ask $question -context "JavaScript" -file $file -model "qwen3-coder:30b"
}
function Ask-PowerShell { 
    param([string]$question, [string]$file) 
    Ask $question -context "PowerShell" -file $file -model "qwen3-coder:30b"
}

# Force complexity levels
function Ask-Simple { param([string]$question) Ask $question -complexity "simple" }
function Ask-Detailed { param([string]$question) Ask $question -complexity "complex" }

# History and utility functions
function Show-AskHistory {
    param([int]$Last = 10)
    
    $recent = $Global:AskHistory | Select-Object -Last $Last
    $recent | ForEach-Object {
        Write-Host "[$($_.Index)] $($_.Timestamp.ToString('HH:mm:ss')) - $($_.Model)" -ForegroundColor Yellow
        Write-Host "Q: $($_.Question)" -ForegroundColor Cyan
        Write-Host "A: $($_.Answer)" -ForegroundColor Green
        if ($_.Thoughts) {
            Write-Host "Thoughts: $($_.Thoughts)" -ForegroundColor Gray
        }
        Write-Host "---" -ForegroundColor DarkGray
    }
}

function Clear-AskHistory {
    $Global:AskHistory = @()
    Write-Host "History cleared." -ForegroundColor Yellow
}

function Get-AskStatus {
    Write-Host "Current Configuration:" -ForegroundColor Yellow
    Write-Host "  Model: $Global:CurrentModel" -ForegroundColor Green
    Write-Host "  Server: $Global:OllamaServer" -ForegroundColor Green
    Write-Host "  History entries: $($Global:AskHistory.Count)" -ForegroundColor Green
    
    # Check Ollama status
    try {
        $models = Invoke-RestMethod -Uri "$Global:OllamaServer/api/tags" -Method Get -TimeoutSec 5
        Write-Host "  Available models: $($models.models.Count)" -ForegroundColor Green
        
        $running = Invoke-RestMethod -Uri "$Global:OllamaServer/api/ps" -Method Get -TimeoutSec 5
        Write-Host "  Running models: $($running.models.Count)" -ForegroundColor Green
    }
    catch {
        Write-Host "  Server status: Offline or unreachable" -ForegroundColor Red
    }
}

# Quick setup function
function Initialize-Ask {
    param(
        [string]$ServerUrl = "http://192.168.50.194:11434",
        [string]$DefaultModel = "fast"
    )
    
    $Global:OllamaServer = $ServerUrl
    Set-AskModel -Model $DefaultModel
    
    Write-Host "Ask function initialized!" -ForegroundColor Green
    Get-AskStatus
}

# Auto-initialize with your server
Initialize-Ask -ServerUrl "http://192.168.50.194:11434"

Write-Host @"
Ask Function Loaded! Available commands:
  Ask "your question"                    - Auto-selects best model
  Ask-Code "write a function"            - Forces coding model  
  Ask-Think "complex analysis" -ShowThoughts - Forces thinking model
  Ask-Fast "simple question"             - Forces fast model
  Ask-Python "python question" -file script.py - Python + coding model
  Set-AskModel fast/code/think          - Set default model
  Show-AskHistory                       - View recent Q&A
  Get-AskStatus                         - Check configuration

Examples:
  Ask "How to reverse a string?"         # Auto → Fast model
  Ask "Create a REST API in Python"      # Auto → Coding model
  Ask "Analyze algorithm complexity"     # Auto → Thinking model
  Ask-Python "Review this code" -file mycode.py
"@ -ForegroundColor Cyan

# Add this function to your PowerShell profile script.
function Ask-Live {
    param(
        [string]$Question,
        [string]$Model = "qwen3:30b-a3b-thinking-2507-q4_K_M"
    )

    # --- 1. SETUP & CONSOLE CONTROL ---
    $ESC = "$([char]27)" # The escape character
    Write-Host "$($ESC)[?25l" -NoNewline # Hide cursor

    # Trap Ctrl+C to ensure we clean up
    $trapAction = {
        Write-Host "$($ESC)[?25h" -NoNewline # Show cursor
        Write-Host "$($ESC)[0m" # Reset colors
        Clear-Host
        Write-Warning "Operation cancelled."
        break
    }
    trap [System.Management.Automation.PipelineStoppedException] {$trapAction}
    trap [System.Management.Automation.ActionPreferenceStopException] {$trapAction}


    # --- 2. DEFINE LAYOUT ---
    Clear-Host
    $width = $Host.UI.RawUI.WindowSize.Width
    $height = $Host.UI.RawUI.WindowSize.Height
    $splitPos = [math]::Floor($width * 0.65)

    # Function to draw the static frame
    function Draw-Frame {
        # Top border
        Write-Host "$($ESC)[1;1H$($ESC)[36m╔$($('═' * ($splitPos - 2)))╦$($('═' * ($width - $splitPos - 1)))╗$($ESC)[0m"
        # Side borders
        for ($i = 2; $i -lt $height; $i++) {
            Write-Host "$($ESC)[${i};1H$($ESC)[36m║$($ESC)[0m"
            Write-Host "$($ESC)[${i};$($splitPos - 1)H$($ESC)[36m║$($ESC)[0m"
            Write-Host "$($ESC)[${i};${width}H$($ESC)[36m║$($ESC)[0m"
        }
        # Bottom border
        Write-Host "$($ESC)[${height};1H$($ESC)[36m╚$($('═' * ($splitPos - 2)))╩$($('═' * ($width - $splitPos - 1)))╝$($ESC)[0m"
        # Titles
        Write-Host "$($ESC)[1;$([math]::Floor(($splitPos-8)/2))H$($ESC)[33m ANSWER $($ESC)[0m"
        Write-Host "$($ESC)[1;$($splitPos + [math]::Floor(($width - $splitPos - 8)/2))H$($ESC)[33m THOUGHTS $($ESC)[0m"
    }

    Draw-Frame

    # --- 3. ASYNCHRONOUS STREAMING ---
    $body = @{
        model = $Model
        prompt = "Always use <think> tags for your reasoning. Question: $Question"
        stream = $true
    } | ConvertTo-Json

    try {
        $response = Invoke-WebRequest -Uri "$Global:OllamaServer/api/generate" -Method Post -Body $body -ContentType "application/json"
        $reader = [System.IO.StreamReader]::new($response.ContentStream)
    }
    catch {
        Write-Host "$($ESC)[?25h" -NoNewline # Show cursor on error
        Write-Error "Failed to connect to Ollama server: $($_.Exception.Message)"
        return
    }

    # --- 4. THE MAIN LOOP ---
    $answerBuffer = ""
    $thoughtsBuffer = ""
    $currentThoughts = ""
    $isThinking = $false

    while (-not $reader.EndOfStream) {
        if ([Console]::KeyAvailable) {
            if (([Console]::ReadKey($true)).Key -eq 'Q') { break }
        }

        $line = $reader.ReadLine()
        if (-not $line) { continue }

        $chunk = $line | ConvertFrom-Json
        $token = $chunk.response

        # Parse content for <think> tags
        if ($token -match '<think>') { $isThinking = $true; continue }
        if ($token -match '</think>') { $isThinking = $false; $thoughtsBuffer += "$currentThoughts`n---`n"; $currentThoughts = ""; continue }

        if ($isThinking) {
            $currentThoughts += $token
        } else {
            $answerBuffer += $token
        }

        # --- REDRAW THE PANES ---
        # Helper for word wrapping and printing
        function Update-Pane {
            param($X, $Y, $PaneWidth, $PaneHeight, $Text, $Color)
            $lines = $Text -split "`n"
            $wrappedLines = @()
            foreach ($line in $lines) {
                for ($i = 0; $i -lt $line.Length; $i += $PaneWidth) {
                    $wrappedLines += $line.Substring($i, [math]::Min($PaneWidth, $line.Length - $i))
                }
            }

            for ($i = 0; $i -lt [math]::Min($wrappedLines.Count, $PaneHeight - 2); $i++) {
                $lineY = $Y + $i
                Write-Host "$($ESC)[${lineY};${X}H$($ESC)[2K" # Move to line and clear it
                Write-Host "$($ESC)[${lineY};${X}H$($Color)$($wrappedLines[$i])$($ESC)[0m"
            }
        }
        
        # Update Answer Pane
        Update-Pane -X 2 -Y 2 -PaneWidth ($splitPos - 3) -PaneHeight $height -Text $answerBuffer -Color "$($ESC)[32m" # Green

        # Update Thoughts Pane
        Update-Pane -X ($splitPos) -Y 2 -PaneWidth ($width - $splitPos - 2) -PaneHeight $height -Text $thoughtsBuffer -Color "$($ESC)[90m" # Gray
    }

    # --- 5. CLEANUP ---
    $reader.Close()
    $response.Close()
    Clear-Host
    Write-Host "$($ESC)[?25h" -NoNewline # Show cursor
    Write-Host "$($ESC)[0m" # Reset colors
    
    # Print the final, clean answer
    Write-Host "=== AI Response ($Model) ===" -ForegroundColor Cyan
    Write-Host $answerBuffer.Trim() -ForegroundColor Green
    Write-Host "============================" -ForegroundColor Cyan
}

#region conda initialize
# !! Contents within this block are managed by 'conda init' !!
(& "C:\Users\jacks\Anaconda3\Scripts\conda.exe" "shell.powershell" "hook") | Out-String | Invoke-Expression
#endregion